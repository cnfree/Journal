# 集群搭建

* 节点不宜过少，需要保证高可用
* 尽量不要用虚拟机，虚拟机无法感知硬盘，有可能一个多份数据跑在同一块硬盘上，导致IO过高，或者硬盘损坏，数据丢失
* 每一台集群节点要保障节点性能参数一致，比如CPU、内存、硬盘、网络等
* 部署集群先要规划好集群节点，比如Job类型、Job任务数，HBase现有容量，Hive容量，HBase表数量，未来每月数据增长量
* HDFS3.0以后不需要每份文件存三份，只需要1.5份做奇偶效验
* Kudu非常耗内存，但千万级数据量用起来比较方便，适合做数仓拉链表，但数据量大的时候不适合
* 实时数据抽取任务需要根据抽取数据量计算内存，最好64G内存
* 数据校验和数据抽取分离，不在同一个任务进
